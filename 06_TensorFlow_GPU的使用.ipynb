{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cd C:\\Program Files\\NVIDIA Corporation\\NVSMI\n",
    "# nvidia-smi.exe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.cnblogs.com/tectal/p/9048184.html\n",
    "# https://blog.csdn.net/fanzonghao/article/details/82218621\n",
    "# https://blog.csdn.net/C_chuxin/article/details/84990176      (tf.ConfigProto和tf.GPUOptions用法总结)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# op 在 cpu 上运算 没装cpu版本\n",
    "#with tf.device('/cpu:0'):\n",
    "#     a = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], shape=[2, 3], name='a')\n",
    "#     b = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], shape=[3, 2], name='b')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# op 在 gpu 上运算\n",
    "with tf.device('/device:GPU:0'):\n",
    "    a = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], shape=[2, 3], name='a')\n",
    "    b = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], shape=[3, 2], na"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# op 在 gpus 上运算 没有多块gpu\n",
    "#for d in ['/device:GPU:2', '/device:GPU:3']:\n",
    "#    with tf.device(d):\n",
    "#        a = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], shape=[2, 3])\n",
    "#        b = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], shape=[3, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 在开启对话session前，先创建一个 tf.ConfigProto() 实例对象\n",
    "# 通过 allow_soft_placement 参数自动将无法放在 GPU 上的操作放回 CPU\n",
    "gpuConfig = tf.ConfigProto(allow_soft_placement=True,log_device_placement=True)# 指定设备不存在时允许TF自动分配设备 是否打印设备分配日志\n",
    "\n",
    "# Case 1\n",
    "# 限定使用显存的比例 一个进程使用 60% 的显存\n",
    "gpuConfig.gpu_options.per_process_gpu_memory_fraction = 0.6\n",
    "# Case 2\n",
    "# 运行时需要多少再给多少\n",
    "gpuConfig.gpu_options.allow_growth = True\n",
    "# Case 3\n",
    "# 指定可以被看见的GPU设备\n",
    "# 默认情况，TF 会占用所有 GPU 的所有内存, 我们可以指定 只有 GPU0 和 GPU1 这两块卡被看到，从而达到限制其使用所有GPU的目的\n",
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0, 1'  \n",
    "\n",
    "# 把你的配置部署到session\n",
    "with tf.Session(config=gpuConfig) as sess:\n",
    "#  pass\n",
    "\n",
    "#这样，如果你指定的卡的显存是8000M的话，你这个进程只能用4800M。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
