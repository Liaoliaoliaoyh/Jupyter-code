{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "old_v = tf.logging.get_verbosity()\n",
    "tf.logging.set_verbosity(tf.logging.ERROR)\n",
    "# OOM : Out Of Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data\\train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data\\train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data\\t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data\\t10k-labels-idx1-ubyte.gz\n",
      "第1次，train准确率 0.720073 test准确率 0.725473\n",
      "第2次，train准确率 0.932727 test准确率 0.937055\n",
      "第3次，train准确率 0.971255 test准确率 0.9716\n",
      "第4次，train准确率 0.978855 test准确率 0.9776\n",
      "第5次，train准确率 0.982236 test准确率 0.980855\n",
      "第6次，train准确率 0.985491 test准确率 0.981836\n",
      "第7次，train准确率 0.987764 test准确率 0.984018\n",
      "第8次，train准确率 0.9892 test准确率 0.985327\n",
      "第9次，train准确率 0.989964 test准确率 0.986\n",
      "第10次，train准确率 0.990982 test准确率 0.986745\n",
      "第11次，train准确率 0.992364 test准确率 0.986673\n",
      "第12次，train准确率 0.993255 test准确率 0.987873\n",
      "第13次，train准确率 0.993691 test准确率 0.987709\n",
      "第14次，train准确率 0.994364 test准确率 0.988091\n",
      "第15次，train准确率 0.995055 test准确率 0.989145\n",
      "第16次，train准确率 0.995655 test准确率 0.988927\n",
      "第17次，train准确率 0.995909 test准确率 0.989309\n",
      "第18次，train准确率 0.996182 test准确率 0.989909\n",
      "第19次，train准确率 0.996327 test准确率 0.989927\n",
      "第20次，train准确率 0.996618 test准确率 0.989455\n",
      "第21次，train准确率 0.997 test准确率 0.990236\n"
     ]
    }
   ],
   "source": [
    "# 载入数据集\n",
    "mnist = input_data.read_data_sets('MNIST_data', one_hot=True)\n",
    "\n",
    "# 每批次训练数据大小 和 批次大小\n",
    "batch_size = 100\n",
    "n_batch = mnist.train.num_examples // batch_size\n",
    "\n",
    "\n",
    "# 初始化权值\n",
    "def weight_variable(shape):\n",
    "    initial = tf.truncated_normal(shape,stddev = 0.1) # 生成一个截断的正态分布\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "# 初始化偏置\n",
    "def bias_variable(shape):\n",
    "    initial = tf.constant(0.1,shape = shape) # 生成一个截断的正态分布\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "# 卷积层\n",
    "# x 是输入一个tensor 四维 [batch批次，长，宽，in_channels通道数（彩色通道数就是3）]\n",
    "# W 是滤波器filter 或者叫卷积核kernel 四维 [滤波器长，宽，输入通道数，输出通道数]\n",
    "# strides步长 要求strides=[0]和strides=[3]都是1 strides=[1]代表x方向步长  strides=[2]代表y方向步长\n",
    "# padding填补的方式 SAME会在外面补0  VALID就是不会补0\n",
    "def conv2d(x,W):\n",
    "    return tf.nn.conv2d(x,W,strides=[1,1,1,1],padding='SAME')\n",
    "\n",
    "# 池化层\n",
    "# ksize是窗口大小 [1,x,y,1]  strides步长同上\n",
    "def max_pooling_2x2(x):\n",
    "    return tf.nn.max_pool(x,ksize=[1,2,2,1],strides=[1,2,2,1],padding='SAME')\n",
    "\n",
    "# 给输入数据预留位置\n",
    "x = tf.placeholder(tf.float32, shape = [None,784])\n",
    "y = tf.placeholder(tf.float32, shape = [None,10])\n",
    "keep_prob = tf.placeholder(tf.float32)    # 神经元的输出概率\n",
    "\n",
    "# 改变x的格式转换为4D的向量 [batch,in_height,in_weight,in_channels]\n",
    "x_image = tf.reshape(x,[-1,28,28,1])\n",
    "\n",
    "# 初始化第一个卷积层的权重和偏置\n",
    "W_conv1 = weight_variable([5,5,1,32])        # 卷积是5×5的采样窗口 1是x_image的in_channels是1维灰度图像 32是使用32个卷积核得到32个特征图\n",
    "b_conv1 = bias_variable([32])                                                     # 每一个卷积核只需要一个偏置值 32个卷积核就是32个偏置值\n",
    "# 把x_image和权值向量进行卷积，再加上偏置值，然后应用于relu激活函数\n",
    "h_conv1 = tf.nn.relu(conv2d(x_image,W_conv1) + b_conv1)                          # 经过卷积操作再加上偏置值 再激活 得到第一个卷积层的结果\n",
    "h_pool1 = max_pooling_2x2(h_conv1)                                                                   #进行max-pooling  得到第一个池化结果\n",
    "\n",
    "\n",
    "# 初始化第二个卷积层的权重和偏置\n",
    "W_conv2 = weight_variable([5,5,32,64])                                    # 5×5的采样窗口 传入32个特征图 用64个卷积核得到 输出64个特征图\n",
    "b_conv2 = bias_variable([64])\n",
    "# 把x_image和权值向量进行卷积，再加上偏置值，然后应用于relu激活函数\n",
    "h_conv2 = tf.nn.relu(conv2d(h_pool1,W_conv2) + b_conv2)\n",
    "h_pool2 = max_pooling_2x2(h_conv2)                                                                   #进行max-pooling\n",
    "h_pool2_flat = tf.reshape(h_pool2,[-1,7*7*64])              # 把池化层2的输出扁平化为1维 是转置吗？ \n",
    "\n",
    "# 读入（reshape）28×28的图片 -->  1卷积结果（方式='SAME'补0）28×28  -->  1池化结果14×14  \n",
    "#                             -->  2卷积结果（方式='SAME'补0）14×14  -->  2池化结果7×7  \n",
    "# 经过上述操作 得到64张7×7的平面\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# 两个全连接层     因为它是全连接层 所以它就只有两个维度\n",
    "\n",
    "W_fc1 = weight_variable([7*7*64,1024])  # 把上面那个平面展开 要7*7*64个神经元 全连接层一共有1024个神经元\n",
    "b_fc1 = bias_variable([1024])           # 1024个偏置\n",
    "# h_pool2_flat = tf.reshape(h_pool2,[-1,7*7*64])\n",
    "h_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat,W_fc1) + b_fc1)  # 求第一个全连接层的输出\n",
    "\n",
    "\n",
    "# keep_prob = tf.placeholder(tf.float32)    # 神经元的输出概率\n",
    "h_fc1_drop = tf.nn.dropout(h_fc1,keep_prob)\n",
    "\n",
    "W_fc2 = weight_variable([1024,10])  # 把上面那个平面展开 要7*7*64个神经元 全连接层一共有1024个神经元\n",
    "b_fc2 = bias_variable([10])           # 1024个偏置\n",
    "\n",
    "# 预测值\n",
    "prediction = tf.nn.softmax(tf.matmul(h_fc1_drop,W_fc2) + b_fc2)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# 交叉熵代价函数\n",
    "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels = y,logits = prediction))\n",
    "# 优化器\n",
    "train = tf.train.AdamOptimizer(0.0001).minimize(loss)\n",
    "\n",
    "# 定义求准确率的方法\n",
    "correct_prediction = tf.equal(tf.argmax(prediction,1), tf.argmax(y,1))     # 返回 Ture False 求出最大的数在的位置 看它俩返回的位置一样不\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction,tf.float32))                                        # cast是强制类型转换 然后 求均值\n",
    "\n",
    "# 初始化变量\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    for epoch in range(21):                                                                                              # 两层循环是训练\n",
    "        train_accuracy_rate = 0\n",
    "        test_accuracy_rate = 0         \n",
    "        for batch in range(n_batch):                               # 每次训练 是从第n个批次里取1个数据 循环结束 所有批次中的数据被训练一次           \n",
    "            batch_data,batch_tag = mnist.train.next_batch(batch_size)   # 从train集中选取batch_size个训练数据 next_batch(我觉得这是bagging 错)         \n",
    "#             sess.run(train,feed_dict = {x:batch_data,y:batch_tag,keep_prob:1.0})                            # 获取一个训练数据 这儿就训练一次   \n",
    "#             train_accuracy_rate += sess.run(accuracy, feed_dict={x:batch_data,y:batch_tag,keep_prob:1.0})\n",
    "            train_acc_rate,_ = sess.run([accuracy,train],feed_dict = {x:batch_data,y:batch_tag,keep_prob:1.0}) \n",
    "            train_accuracy_rate = train_accuracy_rate + train_acc_rate            \n",
    "            batch_test_xs,batch_test_ys = mnist.test.next_batch(batch_size)\n",
    "            test_accuracy_rate += sess.run(accuracy, feed_dict={x: batch_test_xs, y: batch_test_ys,keep_prob:1.0})         \n",
    "        print('第' + str(epoch + 1) + '次，train准确率 ' + str(round(train_accuracy_rate/n_batch,6)) +\n",
    "                                          ' test准确率 ' + str(round(test_accuracy_rate/n_batch,6)))\n",
    "tf.logging.set_verbosity(old_v)\n",
    "\n",
    "\n",
    "\n",
    "# with tf.Session() as sess:\n",
    "#     sess.run(init)\n",
    "#     for epoch in range(11):                                                                                              # 两层循环是训练\n",
    "#         for batch in range(n_batch):                               # 每次训练 是从第n个批次里取1个数据 循环结束 所有批次中的数据被训练一次\n",
    "#             batch_data,batch_tag = mnist.train.next_batch(batch_size)   # 从train集中选取batch_size个训练数据 next_batch(我觉得这是bagging 错)\n",
    "#             sess.run(train,feed_dict = {x:batch_data,y:batch_tag,keep_prob:1.0})                            # 获取一个训练数据 这儿就训练一次   \n",
    "#         train_accuracy_rate = 0\n",
    "#         test_accuracy_rate = 0        \n",
    "#         if epoch % 2 == 0:\n",
    "#             for batch in range(n_batch):\n",
    "#                 batch_test_xs,batch_test_ys = mnist.test.next_batch(batch_size)\n",
    "#                 test_accuracy_rate += sess.run(accuracy, feed_dict={x: batch_test_xs, y: batch_test_ys,keep_prob:1.0})\n",
    "#                 batch_train_xs,batch_train_ys = mnist.train.next_batch(batch_size)\n",
    "#                 train_accuracy_rate += sess.run(accuracy, feed_dict={x: batch_train_xs, y: batch_train_ys,keep_prob:1.0})\n",
    "#             print('第' + str(epoch + 1) + '次，train准确率 ' + str(train_accuracy_rate/(batch+1)) + ' test准确率 ' + str(test_accuracy_rate/(batch+1)))\n",
    "\n",
    "            \n",
    "# tf.logging.set_verbosity(old_v)    \n",
    "    \n",
    "\n",
    "    \n",
    "#          # 每一次大循环 看一下准确率\n",
    "#         if epoch % 1 == 0:\n",
    "# #            train_accuracy_rate = sess.run(accuracy,feed_dict = {x:mnist.train.images,y:mnist.train.labels,keep_prob:1.0})# 训练集正确率\n",
    "#             test_accuracy_rate = sess.run(accuracy,feed_dict = {x:mnist.test.images,y:mnist.test.labels,keep_prob:1.0})   # 测试集正确率\n",
    "# #             print('第' + str(epoch + 1) + '次，train准确率 ' + str(train_accuracy_rate) + ' test准确率 ' + str(test_accuracy_rate))\n",
    "#             print('第' + str(epoch + 1) + '次，test准确率 ' + str(test_accuracy_rate))\n",
    "\n",
    "# tf.logging.set_verbosity(old_v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "549"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "550"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
